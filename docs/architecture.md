# Архитектура RagKotlin

## Компоненты

### OllamaClient
HTTP-клиент для локального сервера Ollama. Генерирует эмбеддинги текста через модель `nomic-embed-text`. Реализует `AutoCloseable` для корректного освобождения ресурсов.

### DeepSeekClient
HTTP-клиент для DeepSeek API. Отправляет запросы к LLM для генерации ответов и реранкинга чанков. Таймаут запроса — 120 секунд. Реализует `AutoCloseable`.

### Индексация (buildIndex)
Читает `.txt` и `.md` файлы из указанных директорий. Разбивает текст на чанки (500 символов с перекрытием 50). Генерирует эмбеддинги для каждого чанка и сохраняет индекс в `index.json`.

### Поиск (findRelevantChunks)
Вычисляет косинусное сходство между эмбеддингом запроса и всеми чанками индекса. Возвращает top-N наиболее релевантных результатов.

### Фильтрация (filterByThreshold)
Отсекает чанки с косинусным сходством ниже заданного порога (по умолчанию 0.72).

### Реранкинг (rerankWithLLM)
Использует LLM для оценки релевантности каждого чанка вопросу. Комбинирует cosine score (40%) и LLM-оценку (60%). Отсекает результаты с комбинированным score ниже 0.5.

### Генерация ответа (askWithContext)
Формирует промпт с контекстом из найденных чанков и историей диалога. LLM генерирует ответ с цитатами и ссылками на источники.

## Поток данных

```
Вопрос пользователя
    │
    ▼
OllamaClient.embed() → эмбеддинг запроса
    │
    ▼
findRelevantChunks() → top-N чанков по cosine similarity
    │
    ▼
filterByThreshold() → отсечение нерелевантных
    │
    ▼
askWithContext() → формирование промпта с контекстом
    │
    ▼
DeepSeekClient.chat() → ответ LLM с источниками
```

## MCP-интеграция

Функция `getCurrentGitBranch()` получает имя текущей git-ветки через `ProcessBuilder` и включает её в системный промпт. Это позволяет ассистенту учитывать контекст разработки.
